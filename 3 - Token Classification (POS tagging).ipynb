{"cells":[{"cell_type":"markdown","id":"1185be39","metadata":{"id":"1185be39"},"source":["# 3 - Частеричная разметка как классификация токенов\n","\n","Основано на [туториале от bentrevett](https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1_bilstm.ipynb).\n","Русскоязычные данные взяты из [корпуса \"Тайга\"](https://tatianashavrina.github.io/taiga_site).\n","\n","## Введение\n","\n","Все задачи обработки текстов (и других последовательностей) можно свести к пяти группам в зависимости от того, что подается на вход и что ожидается на выходе.\n","\n","![](assets/sequence_tasks.jpeg?raw=1)\n","*Источник: [Andrej Karpathy blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n","\n","До этого мы с вами сталкивались только с задачей классификации всей последовательности.\n","\n","В данном туториале мы посмотрим на то, как построить модель с использованием рекуррентных нейронных сетей (а именно двунаправленной LSTM), которая классифицирует каждый элемент входной последовательности. Как пример подобной задачи мы рассмотрим частеричную разметку (part-of-speech (POS) tagging). То же самое может быть применено к задаче выделения именованных сущностей, где для каждого слова генерируется тип сущности (если оно ею является).\n","\n","![](assets/pos_tagging_example.png?raw=1)\n","*Источник: [курс Павла Браславского](https://stepik.org/course/1233)*\n","\n","## Подготовка данных\n","\n","Сначала импортируем необходимые модули."]},{"cell_type":"code","execution_count":null,"id":"3de4e72b","metadata":{"id":"3de4e72b"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchtext\n","from datasets import Dataset, DatasetDict\n","\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","import sys\n","import time\n","import random"]},{"cell_type":"markdown","id":"9d0c3686","metadata":{"id":"9d0c3686"},"source":["Установим единый сид, чтобы результаты были воспроизводимыми."]},{"cell_type":"code","execution_count":null,"id":"0da0eb3c","metadata":{"id":"0da0eb3c"},"outputs":[],"source":["SEED = 42\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","id":"a021652d","metadata":{"id":"a021652d"},"source":["Сначала прочитаем наши данные.\n","Это предложения из статей научно-популярного интернет-издания N+1, которые были размечены тегами UDPOS.\n","Данные находятся в директории `data/pos_nplus1`.\n","Каждое отдельное слово (токен) и соответствующий тег разделенные символом табуляции находятся на отдельной строке.\n","\n","Заведем отдельные списки для токенов и тегов, а затем обернем их в объект класса `Dataset`.\n","Также предусмотрим возможность привести все токены к нижнему регистру.\n","Это может сказаться на качестве модели, но уменьшит её размер."]},{"cell_type":"code","execution_count":null,"id":"bdfb20f0","metadata":{"id":"bdfb20f0"},"outputs":[],"source":["def read_dataset(file_path, lower=True):\n","    tokens, pos_tags = [], []\n","\n","    with open(file_path) as f:\n","        cur_tokens, cur_tags = [], []\n","        for line in f:\n","            line = line.strip()\n","            if not line:\n","                if cur_tokens:\n","                    tokens.append(cur_tokens)\n","                    pos_tags.append(cur_tags)\n","                    cur_tokens, cur_tags = [], []\n","\n","                continue\n","\n","            token, tag = line.split('\\t')\n","            if lower:\n","                token = token.lower()\n","            cur_tokens.append(token)\n","            cur_tags.append(tag)\n","\n","        if cur_tokens:\n","            tokens.append(cur_tokens)\n","            pos_tags.append(cur_tags)\n","\n","    return Dataset.from_dict({'tokens': tokens, 'pos_tags': pos_tags})"]},{"cell_type":"code","execution_count":null,"id":"13072525","metadata":{"id":"13072525"},"outputs":[],"source":["TEXT_LOWER = True"]},{"cell_type":"markdown","id":"c781b807","metadata":{"id":"c781b807"},"source":["Посмотрим на пример объекта класса `Dataset`."]},{"cell_type":"code","execution_count":null,"id":"15d9d82c","metadata":{"id":"15d9d82c"},"outputs":[],"source":["d = read_dataset('data/pos_nplus1/train.txt', lower=TEXT_LOWER)"]},{"cell_type":"markdown","id":"7d0013f1","metadata":{"id":"7d0013f1"},"source":["Объект содержит два поля `tokens` и `pos_tags`. В каждом поле прочитанные нами списки токенов и тегов."]},{"cell_type":"code","execution_count":null,"id":"5d6b984c","metadata":{"id":"5d6b984c"},"outputs":[],"source":["d"]},{"cell_type":"code","execution_count":null,"id":"49fd85f6","metadata":{"id":"49fd85f6"},"outputs":[],"source":["d['tokens'][0]"]},{"cell_type":"code","execution_count":null,"id":"6f41d6d8","metadata":{"id":"6f41d6d8"},"outputs":[],"source":["d['pos_tags'][0]"]},{"cell_type":"code","execution_count":null,"id":"e68e69dc","metadata":{"id":"e68e69dc"},"outputs":[],"source":["for token, tag in zip(d['tokens'][0], d['pos_tags'][0]):\n","    print(f'{token}\\t\\t{tag}')"]},{"cell_type":"markdown","id":"162da25a","metadata":{"id":"162da25a"},"source":["Посмотрим на распределение тегов в нашей тренировочной выборке."]},{"cell_type":"code","execution_count":null,"id":"0adb5c44","metadata":{"id":"0adb5c44"},"outputs":[],"source":["tags_counter = Counter()\n","\n","for tags in d['pos_tags']:\n","    tags_counter.update(tags)"]},{"cell_type":"code","execution_count":null,"id":"5a674af2","metadata":{"id":"5a674af2"},"outputs":[],"source":["n_train_tags = sum(tags_counter.values())\n","\n","for tag, tag_count in tags_counter.most_common():\n","    print(f'{tag}\\t{tag_count / n_train_tags * 100: .2f}%')"]},{"cell_type":"markdown","id":"e40cb8f5","metadata":{"id":"e40cb8f5"},"source":["Прочитаем все выборки нашего датасета и обернем их в объект класса `DatasetDict`. По сути это словарь для датасетов, позволяющий сразу применять действия к нескольким объектам `Dataset`."]},{"cell_type":"code","execution_count":null,"id":"f88f9303","metadata":{"id":"f88f9303"},"outputs":[],"source":["data = DatasetDict()\n","\n","for split_name in ['train', 'validation', 'test']:\n","    data[split_name] = read_dataset(f'data/pos_nplus1/{split_name}.txt', lower=TEXT_LOWER)"]},{"cell_type":"code","execution_count":null,"id":"d698612c","metadata":{"id":"d698612c"},"outputs":[],"source":["data"]},{"cell_type":"markdown","id":"daeadfcd","metadata":{"id":"daeadfcd"},"source":["Теперь построим словари (vocabulary) для наших полей. Словарь в данном случае -- это сопоставление каждому токену (тегу, лейблу) уникального индекса.\n","\n","Словари строятся на основе тренировочной выборки, из-за чего на этапе валидации/эксплуатации могут встречаться незнакомые токены. Такие токены будут заменены на специальный токен `<unk>`. Чтобы снизить количество параметров и подготовить модель к встрече с незнакомыми словами мы установим `min_freq = 2`, из-за чего в словарь модели будут добавляться токены, которые встречаются минимум 2 раза. Также в наши словари будет добавлен специальный токен `<pad>`, который пригодится нам позднее."]},{"cell_type":"code","execution_count":null,"id":"d1c83685","metadata":{"id":"d1c83685"},"outputs":[],"source":["MIN_FREQ = 2\n","\n","tokens_vocab = torchtext.vocab.build_vocab_from_iterator(data['train']['tokens'], min_freq=MIN_FREQ,\n","                                                         specials=['<unk>', '<pad>'])\n","\n","tags_vocab = torchtext.vocab.build_vocab_from_iterator(data['train']['pos_tags'], specials=['<pad>'])"]},{"cell_type":"markdown","id":"fe796d28","metadata":{"id":"fe796d28"},"source":["Посмотрим на размеры словарей и примеры."]},{"cell_type":"code","execution_count":null,"id":"ec59fbd5","metadata":{"id":"ec59fbd5"},"outputs":[],"source":["len(tokens_vocab), len(tags_vocab)"]},{"cell_type":"code","execution_count":null,"id":"043d66ce","metadata":{"id":"043d66ce"},"outputs":[],"source":["tokens_vocab['ученые']"]},{"cell_type":"code","execution_count":null,"id":"dab5f8ea","metadata":{"id":"dab5f8ea"},"outputs":[],"source":["tags_vocab['VERB']"]},{"cell_type":"markdown","id":"fdbb3277","metadata":{"id":"fdbb3277"},"source":["Укажем для словаря токенов, чтобы при появлении незнакомого слова возвращался индекс токена `<unk>`."]},{"cell_type":"code","execution_count":null,"id":"d379bbaa","metadata":{"scrolled":false,"id":"d379bbaa"},"outputs":[],"source":["# tokens_vocab['blablabla']"]},{"cell_type":"code","execution_count":null,"id":"3c1c24c5","metadata":{"id":"3c1c24c5"},"outputs":[],"source":["tokens_vocab.set_default_index(tokens_vocab['<unk>'])"]},{"cell_type":"code","execution_count":null,"id":"545ef4a0","metadata":{"id":"545ef4a0"},"outputs":[],"source":["tokens_vocab['blablabla'], tokens_vocab['<unk>']"]},{"cell_type":"markdown","id":"97f1cc8e","metadata":{"id":"97f1cc8e"},"source":["Мы можем получить полное сопостовление между токенами (тегами) и их индексами в словаре."]},{"cell_type":"code","execution_count":null,"id":"5eb8977e","metadata":{"id":"5eb8977e"},"outputs":[],"source":["idx_to_tag = tags_vocab.vocab.get_itos()\n","idx_to_tag"]},{"cell_type":"code","execution_count":null,"id":"7acefa08","metadata":{"id":"7acefa08"},"outputs":[],"source":["tag_to_idx = tags_vocab.vocab.get_stoi()\n","tag_to_idx"]},{"cell_type":"markdown","id":"d7014364","metadata":{"id":"d7014364"},"source":["Но удобнее пользоваться готовыми функциями, для преобразования нескольких токенов (тегов) в индексы и обратно."]},{"cell_type":"code","execution_count":null,"id":"c8c00a07","metadata":{"id":"c8c00a07"},"outputs":[],"source":["token_idxs_example = tokens_vocab.forward(data['train']['tokens'][42])\n","tag_idxs_example = tags_vocab.forward(data['train']['pos_tags'][42])"]},{"cell_type":"code","execution_count":null,"id":"20525372","metadata":{"id":"20525372"},"outputs":[],"source":["print(token_idxs_example)"]},{"cell_type":"code","execution_count":null,"id":"ef89cb5e","metadata":{"id":"ef89cb5e"},"outputs":[],"source":["print(tag_idxs_example)"]},{"cell_type":"code","execution_count":null,"id":"f180804b","metadata":{"id":"f180804b"},"outputs":[],"source":["print(tokens_vocab.lookup_tokens(token_idxs_example))\n","print(data['train']['tokens'][42])"]},{"cell_type":"code","execution_count":null,"id":"851ae372","metadata":{"id":"851ae372"},"outputs":[],"source":["print(tags_vocab.lookup_tokens(tag_idxs_example))\n","print(data['train']['pos_tags'][42])"]},{"cell_type":"markdown","id":"5259aada","metadata":{"id":"5259aada"},"source":["Теперь, наконец, переведем все наши данные в числовой формат."]},{"cell_type":"code","execution_count":null,"id":"c76c6647","metadata":{"id":"c76c6647"},"outputs":[],"source":["def numericalize_data(example):\n","    token_idxs = tokens_vocab.forward(example['tokens'])\n","    tag_idxs = tags_vocab.forward(example['pos_tags'])\n","    return {'token_idxs': token_idxs, 'tag_idxs': tag_idxs}"]},{"cell_type":"markdown","id":"209970ce","metadata":{"id":"209970ce"},"source":["Применяем функцию `numericalize_data` ко всем данным, удаляем ненужные колонки и переводим данные в тип `torch.Tensor`."]},{"cell_type":"code","execution_count":null,"id":"db34a655","metadata":{"id":"db34a655"},"outputs":[],"source":["transformed_data = data.map(numericalize_data, remove_columns=['tokens', 'pos_tags']).with_format(type='torch')"]},{"cell_type":"code","execution_count":null,"id":"2bfe5752","metadata":{"id":"2bfe5752"},"outputs":[],"source":["transformed_data"]},{"cell_type":"code","execution_count":null,"id":"04979ac5","metadata":{"id":"04979ac5"},"outputs":[],"source":["transformed_data['train']['token_idxs'][0]"]},{"cell_type":"code","execution_count":null,"id":"818cbbf3","metadata":{"id":"818cbbf3"},"outputs":[],"source":["transformed_data['train']['tag_idxs'][0]"]},{"cell_type":"markdown","id":"9490702f","metadata":{"id":"9490702f"},"source":["Последний этап предобработки данных -- это создание итераторов, которые будут перемешивать данные и делить их на батчи. За это отвечают объекты класса `DataLoader`. При создании итераторов будем передавать функцию `collate_batch`, которая принимает на вход фрагмент датасета и строит два списка: список токенов (точнее их индексов) и список тегов.\n","\n","В то время как предложения обычно имеют разную длину, входные и выходные последовательности  представлены в виде трехмерного тензора. Поэтому все короткие предложения в батче приводятся к длине самого длинного предложения путем добавления токена `<pad>` (точнее его индекса)."]},{"cell_type":"code","execution_count":null,"id":"7acffd3f","metadata":{"id":"7acffd3f"},"outputs":[],"source":["def collate_batch(batch):\n","    batch_tokens = [example['token_idxs'] for example in batch]\n","    batch_tags = [example['tag_idxs'] for example in batch]\n","    batch_tokens = nn.utils.rnn.pad_sequence(batch_tokens, padding_value=tokens_vocab['<pad>'], batch_first=True)\n","    batch_tags = nn.utils.rnn.pad_sequence(batch_tags, padding_value=tags_vocab['<pad>'], batch_first=True)\n","    batch = {'token_idxs': batch_tokens,\n","             'tag_idxs': batch_tags}\n","    return batch"]},{"cell_type":"code","execution_count":null,"id":"1e042de2","metadata":{"id":"1e042de2"},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","train_dataloader = torch.utils.data.DataLoader(transformed_data['train'],\n","                                               batch_size=BATCH_SIZE,\n","                                               collate_fn=collate_batch,\n","                                               shuffle=True)\n","\n","validation_dataloader = torch.utils.data.DataLoader(transformed_data['validation'],\n","                                                    batch_size=BATCH_SIZE,\n","                                                    collate_fn=collate_batch)\n","\n","test_dataloader = torch.utils.data.DataLoader(transformed_data['test'],\n","                                              batch_size=BATCH_SIZE,\n","                                              collate_fn=collate_batch)"]},{"cell_type":"markdown","id":"09a4c056","metadata":{"id":"09a4c056"},"source":["Посмотрим на пример батча:"]},{"cell_type":"code","execution_count":null,"id":"18686a73","metadata":{"id":"18686a73"},"outputs":[],"source":["for batch in train_dataloader:\n","    break"]},{"cell_type":"code","execution_count":null,"id":"c8ee5e70","metadata":{"id":"c8ee5e70"},"outputs":[],"source":["batch"]},{"cell_type":"markdown","id":"5256b8df","metadata":{"id":"5256b8df"},"source":["Или так:"]},{"cell_type":"code","execution_count":null,"id":"31852c8e","metadata":{"id":"31852c8e"},"outputs":[],"source":["next(iter(train_dataloader))"]},{"cell_type":"markdown","id":"a532b657","metadata":{"id":"a532b657"},"source":["## Строим модель"]},{"cell_type":"markdown","id":"cfa5a831","metadata":{"id":"cfa5a831"},"source":["Следующий шаг -- построить нашу модель. Модель будет включать два слоя двунаправленной (значит обрабатывать текст справа налево и слева направо) рекуррентной нейронной сети.\n","\n","При этом в качестве рекуррентного слоя будет выступать не простая RNN, как мы реализовывали в прошлом ноутбуке, а LSTM. Основным недостатком классической RNN является то, что она обладает высокой забывчивостью. Для решения этой проблемы в LSTM был реализован дополнительный вектор состояния ячейки, который сеть на каждом шаге обновляет.\n","\n","![](assets/forgetting.jpg)\n","*Источник: [LSTM – сети долгой краткосрочной памяти](https://habr.com/ru/companies/wunderfund/articles/331310/)*\n","\n","\n","![](assets/lstm.png)\n","*Источник: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)*"]},{"cell_type":"markdown","id":"f56e0524","metadata":{"id":"f56e0524"},"source":["![](assets/pos-bidirectional-lstm.png?raw=1)\n","\n","Модель получает на вход последовательность индексов токенов $X = \\{x_1, x_2,...,x_T\\}$, и проводит их через слой эмбеддингов $e$. Таким обазом получаем набор эмбеддингов для каждого слова: $e(X) = \\{e(x_1), e(x_2), ..., e(x_T)\\}$.\n","\n","Эти эмбеддинги обрабатываются по одному в один момент времени: по прямой и обратной LSTM. Прямая LSTM обрабатывает последовательность слева направо, а обратная -- справо налево. То есть первый токен прямой LSTM -- $x_1$, первый токен обратной -- $x_T$.\n","\n","Также LSTM получает а вход скрытое состояние $h$ и состояние ячейки $c$, полученные после обработки предыдущего слова.\n","\n","$$h^{\\rightarrow}_t = \\text{LSTM}^{\\rightarrow}(e(x^{\\rightarrow}_t), h^{\\rightarrow}_{t-1}, c^{\\rightarrow}_{t-1})$$\n","$$h^{\\leftarrow}_t=\\text{LSTM}^{\\leftarrow}(e(x^{\\leftarrow}_t), h^{\\leftarrow}_{t-1}, c^{\\leftarrow}_{t-1})$$\n","\n","После того, как последовательность была полностью обработана, скрытое состояние и состояние ячейчки передаются в следующий слой LSTM.\n","\n","Начальные значения $h_0$ и $c_0$ инициализируются тензором из нулей.\n","\n","\n","Все скрытыте состояния после прямого и обратного прохода на последнем слое LSTM конкатенируются: $H = \\{h_1, h_2, ... h_T\\}$, где $h_1 = [h^{\\rightarrow}_1;h^{\\leftarrow}_T]$, $h_2 = [h^{\\rightarrow}_2;h^{\\leftarrow}_{T-1}]$ и т.д. и подаются на вход линейному слою $f$, который используется для предсказания того, какой тег соответствует токену: $\\hat{y}_t = f(h_t)$.\n","\n","В процессе тренировки модели мы будем сравнивать наши предсказанные теги $\\hat{Y}$ с тегами из датасета $Y$, вычислять значение функции ошибки, вычислять градиенты и обновлять наши параметры.\n","\n","`nn.Embedding` слой эмбеддингов, input_dim которого должен быть равен размеру словаря, а embedding_dim является подбираемым гиперпараметром. Также мы сообщаем слою индекс <pad> токена, чтобы не обновлять его эмбеддинг.\n","\n","`nn.LSTM` слой LSTM. Если у нас несколько слоев LSTM, то между ними применяется дропаут.\n","\n","`nn.Linear` определяет линейный слой, который делает предсказания с использованием выходов из LSTM. Если из LSTM мы ожидаем скрытые состояния после прямого и обратного прохода, то вход линейного слоя удваивается. Выход из линейного слоя должен быть равен размеру словаря тегов.\n","\n","Также в качестве регуляризации в процессе обучения мы применяем слой `nn.Dropout` к эмбеддингам и выходам из LSTM."]},{"cell_type":"code","execution_count":null,"id":"15fc8e6a","metadata":{"id":"15fc8e6a"},"outputs":[],"source":["class BiLSTMPOSTagger(nn.Module):\n","    def __init__(self,\n","                 input_dim,\n","                 embedding_dim,\n","                 hidden_dim,\n","                 output_dim,\n","                 n_layers,\n","                 bidirectional,\n","                 dropout,\n","                 pad_idx):\n","\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n","\n","        self.lstm = nn.LSTM(embedding_dim,\n","                            hidden_dim,\n","                            num_layers = n_layers,\n","                            bidirectional = bidirectional,\n","                            dropout = dropout if n_layers > 1 else 0)\n","\n","        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text):\n","\n","        #text = [sent len, batch size]\n","\n","        #pass text through embedding layer\n","        embedded = self.dropout(self.embedding(text))\n","\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        #pass embeddings into LSTM\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","\n","        #outputs holds the backward and forward hidden states in the final layer\n","        #hidden and cell are the backward and forward hidden and cell states at the final time-step\n","\n","        #output = [sent len, batch size, hid dim * n directions]\n","        #hidden/cell = [n layers * n directions, batch size, hid dim]\n","\n","        #we use our outputs to make a prediction of what the tag should be\n","        predictions = self.fc(self.dropout(outputs))\n","\n","        #predictions = [sent len, batch size, output dim]\n","\n","        return predictions"]},{"cell_type":"markdown","id":"9154b7e0","metadata":{"id":"9154b7e0"},"source":["## Обучаем модель"]},{"cell_type":"code","execution_count":null,"id":"c7f371c2","metadata":{"id":"c7f371c2"},"outputs":[],"source":["INPUT_DIM = len(tokens_vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 128\n","OUTPUT_DIM = len(tags_vocab)\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.25\n","TOKEN_PAD_IDX = tokens_vocab['<pad>']\n","\n","model = BiLSTMPOSTagger(INPUT_DIM,\n","                        EMBEDDING_DIM,\n","                        HIDDEN_DIM,\n","                        OUTPUT_DIM,\n","                        N_LAYERS,\n","                        BIDIRECTIONAL,\n","                        DROPOUT,\n","                        TOKEN_PAD_IDX)"]},{"cell_type":"code","execution_count":null,"id":"a83429fa","metadata":{"id":"a83429fa"},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":null,"id":"359a8064","metadata":{"id":"359a8064"},"outputs":[],"source":["for p in model.parameters():\n","    print(p)\n","    break"]},{"cell_type":"markdown","id":"20851964","metadata":{"id":"20851964"},"source":["Инициализируем веса из нормального распределения."]},{"cell_type":"code","execution_count":null,"id":"065f376d","metadata":{"id":"065f376d"},"outputs":[],"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.normal_(param.data, mean = 0, std = 0.1)\n","\n","model.apply(init_weights)"]},{"cell_type":"markdown","id":"93c626c1","metadata":{"id":"93c626c1"},"source":["Считаем, сколько параметров в нашей модели:"]},{"cell_type":"code","execution_count":null,"id":"2b9edc21","metadata":{"id":"2b9edc21"},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'В модели {count_parameters(model):,} обучаемых параметров')"]},{"cell_type":"code","execution_count":null,"id":"3d8bb0e5","metadata":{"id":"3d8bb0e5"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters())"]},{"cell_type":"code","execution_count":null,"id":"5da92cdb","metadata":{"id":"5da92cdb"},"outputs":[],"source":["TAG_PAD_IDX = tags_vocab['<pad>']\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"]},{"cell_type":"code","execution_count":null,"id":"b3b77ef9","metadata":{"id":"b3b77ef9"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":null,"id":"b7c66929","metadata":{"id":"b7c66929"},"outputs":[],"source":["model = model.to(device)\n","criterion = criterion.to(device)"]},{"cell_type":"code","execution_count":null,"id":"8a13deac","metadata":{"id":"8a13deac"},"outputs":[],"source":["def categorical_accuracy(preds, y, tag_pad_idx):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n","    non_pad_elements = (y != tag_pad_idx).nonzero()\n","    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n","    return correct.sum() / y[non_pad_elements].shape[0]"]},{"cell_type":"code","execution_count":null,"id":"1cc198f5","metadata":{"id":"1cc198f5"},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion, device, tag_pad_idx):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","\n","    for batch in tqdm(dataloader, desc='training...', file=sys.stdout):\n","        text = batch['token_idxs'].to(device)\n","        tags = batch['tag_idxs'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        #text = [sent len, batch size]\n","\n","        predictions = model(text)\n","\n","        #predictions = [sent len, batch size, output dim]\n","        #tags = [sent len, batch size]\n","\n","        predictions = predictions.view(-1, predictions.shape[-1])\n","        tags = tags.view(-1)\n","\n","        #predictions = [sent len * batch size, output dim]\n","        #tags = [sent len * batch size]\n","\n","        loss = criterion(predictions, tags)\n","\n","        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","    return (epoch_loss / len(dataloader), epoch_acc / len(dataloader))"]},{"cell_type":"code","execution_count":null,"id":"9535914c","metadata":{"id":"9535914c"},"outputs":[],"source":["def evaluate(model, dataloader, criterion, device, tag_pad_idx):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for batch in tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n","\n","            text = batch['token_idxs'].to(device)\n","            tags = batch['tag_idxs'].to(device)\n","\n","            predictions = model(text)\n","\n","            predictions = predictions.view(-1, predictions.shape[-1])\n","            tags = tags.view(-1)\n","\n","            loss = criterion(predictions, tags)\n","\n","            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","    return (epoch_loss / len(dataloader), epoch_acc / len(dataloader))"]},{"cell_type":"code","execution_count":null,"id":"d8f8a901","metadata":{"id":"d8f8a901"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":null,"id":"05664ca8","metadata":{"id":"05664ca8"},"outputs":[],"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    _, _ = train(model, train_dataloader, optimizer, criterion, device, TAG_PAD_IDX)\n","    epoch_train_loss, epoch_train_acc = evaluate(model, train_dataloader, criterion, device, TAG_PAD_IDX)\n","    epoch_valid_loss, epoch_valid_acc = evaluate(model, validation_dataloader, criterion, device, TAG_PAD_IDX)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if epoch_valid_loss < best_valid_loss:\n","        best_valid_loss = epoch_valid_loss\n","        torch.save(model.state_dict(), 'tut1-model.pt')\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {epoch_train_loss:.3f} | Train Acc: {epoch_train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {epoch_valid_loss:.3f} |  Val. Acc: {epoch_valid_acc*100:.2f}%')"]},{"cell_type":"markdown","id":"42b42b85","metadata":{"id":"42b42b85"},"source":["## Оцениваем модель на различные данных"]},{"cell_type":"code","execution_count":null,"id":"94a62076","metadata":{"scrolled":true,"id":"94a62076"},"outputs":[],"source":["model.load_state_dict(torch.load('tut1-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_dataloader, criterion, device, TAG_PAD_IDX)\n","\n","print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"]},{"cell_type":"code","execution_count":null,"id":"12a6691f","metadata":{"id":"12a6691f"},"outputs":[],"source":["import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"id":"a57aefb3","metadata":{"id":"a57aefb3"},"outputs":[],"source":["def tag_sentence(model, device, sentence, tokens_vocab, tags_vocab, lower=True):\n","\n","    model.eval()\n","\n","    if isinstance(sentence, str):\n","        tokens = word_tokenize(sentence, language='russian')\n","    else:\n","        tokens = sentence\n","\n","    if lower:\n","        tokens = [token.lower() for token in tokens]\n","\n","    numericalized_tokens = tokens_vocab.forward(tokens)\n","\n","    unk_idx = tokens_vocab['<unk>']\n","\n","    unks = [token for token, token_idx in zip(tokens, numericalized_tokens) if token_idx == unk_idx]\n","\n","    token_tensor = torch.LongTensor(numericalized_tokens)\n","\n","    token_tensor = token_tensor.unsqueeze(-1).to(device)\n","\n","    predictions = model(token_tensor)\n","\n","    top_predictions = predictions.argmax(-1)\n","\n","    predicted_tags = tags_vocab.lookup_tokens(top_predictions.cpu().numpy())\n","\n","    return tokens, predicted_tags, unks"]},{"cell_type":"code","execution_count":null,"id":"e651cb0b","metadata":{"id":"e651cb0b"},"outputs":[],"source":["example_index = 1\n","\n","sentence = data['test']['tokens'][example_index]\n","actual_tags = data['test']['pos_tags'][example_index]\n","\n","print(sentence)"]},{"cell_type":"code","execution_count":null,"id":"b44dd02f","metadata":{"id":"b44dd02f"},"outputs":[],"source":["tokens, pred_tags, unks = tag_sentence(model, device, sentence, tokens_vocab, tags_vocab, TEXT_LOWER)\n","\n","print(unks)"]},{"cell_type":"code","execution_count":null,"id":"3c5916e3","metadata":{"id":"3c5916e3"},"outputs":[],"source":["print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n","\n","for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n","    correct = '✔' if pred_tag == actual_tag else '✘'\n","    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"]},{"cell_type":"code","execution_count":null,"id":"d36c1aec","metadata":{"id":"d36c1aec"},"outputs":[],"source":["all_actual_tags = np.hstack(data['test']['pos_tags'])\n","all_predicted_tags = np.array([])\n","\n","for sentence in tqdm(data['test']['tokens']):\n","    _, pred_tags, _ = tag_sentence(model, device, sentence, tokens_vocab, tags_vocab, TEXT_LOWER)\n","    all_predicted_tags = np.hstack((all_predicted_tags, pred_tags))"]},{"cell_type":"code","execution_count":null,"id":"354c6bcb","metadata":{"id":"354c6bcb"},"outputs":[],"source":["len(all_actual_tags), len(all_predicted_tags)"]},{"cell_type":"code","execution_count":null,"id":"67d75773","metadata":{"id":"67d75773"},"outputs":[],"source":["labels = tags_vocab.get_itos()\n","labels.remove('<pad>')"]},{"cell_type":"code","execution_count":null,"id":"3b2af3dd","metadata":{"id":"3b2af3dd"},"outputs":[],"source":["cm = confusion_matrix(all_actual_tags, all_predicted_tags, labels=labels, normalize='true') # , normalize='pred'"]},{"cell_type":"code","execution_count":null,"id":"c845bdab","metadata":{"id":"c845bdab"},"outputs":[],"source":["cm"]},{"cell_type":"code","execution_count":null,"id":"595fc147","metadata":{"id":"595fc147"},"outputs":[],"source":["cmd_obj = ConfusionMatrixDisplay(cm, display_labels=labels)\n","cmd_obj.plot(include_values=False, xticks_rotation='vertical')"]},{"cell_type":"code","execution_count":null,"id":"e9afdce7","metadata":{"id":"e9afdce7"},"outputs":[],"source":["sentence = 'Сегодня анализ текста применяется во многих областях нашей жизни — от абсолютно повседневных вещей \\\n","            вроде поисковых систем и Гугл-переводчика до нейросетей.'"]},{"cell_type":"code","execution_count":null,"id":"a070d0cd","metadata":{"id":"a070d0cd"},"outputs":[],"source":["tokens, pred_tags, unks = tag_sentence(model, device, sentence, tokens_vocab, tags_vocab, TEXT_LOWER)\n","\n","print(unks)"]},{"cell_type":"code","execution_count":null,"id":"10b955d7","metadata":{"id":"10b955d7"},"outputs":[],"source":["print(\"Pred. Tag\\tToken\\n\")\n","\n","for token, tag in zip(tokens, pred_tags):\n","    print(f\"{tag}\\t\\t{token}\")"]},{"cell_type":"code","execution_count":null,"id":"70ee7697","metadata":{"id":"70ee7697"},"outputs":[],"source":["sentence = 'Мистер Шерлок Холмс, имевший обыкновение вставать очень поздно, \\\n","            за исключением тех нередких случаев, когда вовсе не ложился спать, сидел за завтраком.'"]},{"cell_type":"code","execution_count":null,"id":"2dedb14f","metadata":{"id":"2dedb14f"},"outputs":[],"source":["tokens, pred_tags, unks = tag_sentence(model, device, sentence, tokens_vocab, tags_vocab, TEXT_LOWER)\n","\n","print(unks)"]},{"cell_type":"code","execution_count":null,"id":"59ce9235","metadata":{"id":"59ce9235"},"outputs":[],"source":["print(\"Pred. Tag\\tToken\\n\")\n","\n","for token, tag in zip(tokens, pred_tags):\n","    print(f\"{tag}\\t\\t{token}\")"]},{"cell_type":"code","execution_count":null,"id":"1779919b","metadata":{"id":"1779919b"},"outputs":[],"source":["other_domain = read_dataset('data/pos_stihiru/test.txt', TEXT_LOWER)\n","other_domain"]},{"cell_type":"code","execution_count":null,"id":"07dad3c5","metadata":{"id":"07dad3c5"},"outputs":[],"source":["transformed_other = other_domain.map(numericalize_data, remove_columns=['tokens', 'pos_tags']).with_format(type='torch')\n","transformed_other"]},{"cell_type":"code","execution_count":null,"id":"7b5c7f64","metadata":{"id":"7b5c7f64"},"outputs":[],"source":["other_dataloader = torch.utils.data.DataLoader(transformed_other,\n","                                              batch_size=BATCH_SIZE,\n","                                              collate_fn=collate_batch)"]},{"cell_type":"code","execution_count":null,"id":"43459400","metadata":{"id":"43459400"},"outputs":[],"source":["model.load_state_dict(torch.load('tut1-model.pt'))\n","\n","other_loss, other_acc = evaluate(model, other_dataloader, criterion, device, TAG_PAD_IDX)\n","\n","print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"]},{"cell_type":"code","execution_count":null,"id":"f58a22e8","metadata":{"id":"f58a22e8"},"outputs":[],"source":["example_index = 1\n","\n","sentence = other_domain['tokens'][example_index]\n","actual_tags = other_domain['pos_tags'][example_index]\n","\n","print(sentence)"]},{"cell_type":"code","execution_count":null,"id":"76717d10","metadata":{"id":"76717d10"},"outputs":[],"source":["tokens, pred_tags, unks = tag_sentence(model, device, sentence, tokens_vocab, tags_vocab, TEXT_LOWER)\n","\n","print(unks)"]},{"cell_type":"code","execution_count":null,"id":"e9c302db","metadata":{"id":"e9c302db"},"outputs":[],"source":["print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n","\n","for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n","    correct = '✔' if pred_tag == actual_tag else '✘'\n","    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"]},{"cell_type":"code","execution_count":null,"id":"2a88ff03","metadata":{"id":"2a88ff03"},"outputs":[],"source":["all_actual_tags = np.hstack(other_domain['pos_tags'])\n","all_predicted_tags = np.array([])\n","\n","for sentence in tqdm(other_domain['tokens']):\n","    _, pred_tags, _ = tag_sentence(model, device, sentence, tokens_vocab, tags_vocab, TEXT_LOWER)\n","    all_predicted_tags = np.hstack((all_predicted_tags, pred_tags))"]},{"cell_type":"code","execution_count":null,"id":"ad45f58b","metadata":{"id":"ad45f58b"},"outputs":[],"source":["len(all_actual_tags), len(all_predicted_tags)"]},{"cell_type":"code","execution_count":null,"id":"d72f864b","metadata":{"id":"d72f864b"},"outputs":[],"source":["labels = tags_vocab.get_itos()\n","labels.remove('<pad>')"]},{"cell_type":"code","execution_count":null,"id":"df8ef531","metadata":{"id":"df8ef531"},"outputs":[],"source":["cm = confusion_matrix(all_actual_tags, all_predicted_tags, labels=labels, normalize='true')"]},{"cell_type":"code","execution_count":null,"id":"397bb75f","metadata":{"id":"397bb75f"},"outputs":[],"source":["cmd_obj = ConfusionMatrixDisplay(cm, display_labels=labels)\n","cmd_obj.plot(include_values=False, xticks_rotation='vertical')"]},{"cell_type":"code","execution_count":null,"id":"2cbfa287","metadata":{"id":"2cbfa287"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}